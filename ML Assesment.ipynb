{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04ed39a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Future exception was never retrieved\n",
      "future: <Future finished exception=NetworkError('Protocol error (Target.sendMessageToTarget): No session with given id')>\n",
      "pyppeteer.errors.NetworkError: Protocol error (Target.sendMessageToTarget): No session with given id\n",
      "Future exception was never retrieved\n",
      "future: <Future finished exception=NetworkError('Protocol error (Target.sendMessageToTarget): No session with given id')>\n",
      "pyppeteer.errors.NetworkError: Protocol error (Target.sendMessageToTarget): No session with given id\n",
      "Future exception was never retrieved\n",
      "future: <Future finished exception=NetworkError('Protocol error (Target.sendMessageToTarget): No session with given id')>\n",
      "pyppeteer.errors.NetworkError: Protocol error (Target.sendMessageToTarget): No session with given id\n",
      "Future exception was never retrieved\n",
      "future: <Future finished exception=NetworkError('Protocol error (Target.sendMessageToTarget): No session with given id')>\n",
      "pyppeteer.errors.NetworkError: Protocol error (Target.sendMessageToTarget): No session with given id\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from requests_html import AsyncHTMLSession\n",
    "from pyppeteer import launch\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Function to scrape news articles from Hindustan Times\n",
    "async def scrape_hindustan_times():\n",
    "    url = 'https://www.hindustantimes.com/'\n",
    "\n",
    "    # Create an asynchronous HTML session\n",
    "    session = AsyncHTMLSession()\n",
    "\n",
    "    # Use async functions when working with the session\n",
    "    r = await session.get(url)\n",
    "\n",
    "    # render the HTML to execute JavaScript\n",
    "    await r.html.arender()\n",
    "\n",
    "    # Use pyppeteer for scrolling\n",
    "    browser = await launch()\n",
    "    page = await browser.newPage()\n",
    "    await page.goto(url)\n",
    "\n",
    "    # Scroll down to load more sections\n",
    "    for i in range(5):  # You may need to adjust the range based on the actual number of scrolls needed\n",
    "        await page.evaluate(\"window.scrollBy(0, window.innerHeight);\")\n",
    "        await asyncio.sleep(1)  # Adjust the sleep duration if necessary\n",
    "\n",
    "    # Extracting article text from different sections\n",
    "    articles_by_section = []\n",
    "\n",
    "    # Replace 'leftFixedNav' with the actual class used for the container that encompasses all sections\n",
    "    for section_container in r.html.find('.leftFixedNav'):\n",
    "        # Replace 'li' with the actual tag used for each section within the container\n",
    "        for section in section_container.find('li'):\n",
    "            section_name = section.find('a', first=True).text.strip()\n",
    "            section_link = section.find('a', first=True).attrs['href']\n",
    "            section_articles = await extract_articles_from_section(session, section_link)\n",
    "            articles_by_section.extend([(article_title, section_name) for article_title in section_articles])\n",
    "\n",
    "    # Close the session and browser\n",
    "    await session.close()\n",
    "    await browser.close()\n",
    "\n",
    "    # Create a DataFrame from the extracted articles\n",
    "    df = pd.DataFrame(articles_by_section, columns=['article', 'section'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Function to extract articles from a section page\n",
    "async def extract_articles_from_section(session, section_link):\n",
    "    r = await session.get(section_link)\n",
    "    await r.html.arender()\n",
    "\n",
    "    # Replace 'your_article_selector' with the appropriate selector for the article links on the section pages\n",
    "    article_elements = r.html.find('h3 a')  # Assuming the article links are within h3 tags\n",
    "\n",
    "    # Extract article titles\n",
    "    articles = [article_element.text for article_element in article_elements]\n",
    "\n",
    "    return articles\n",
    "\n",
    "# Allow nested asyncio calls in Jupyter notebook\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Run the event loop to execute the asynchronous code\n",
    "df_result = asyncio.run(scrape_hindustan_times())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "753b8db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the scraped data in a CSV file\n",
    "df_result.to_csv('scraped_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b80ee8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.24\n",
      "\n",
      "Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    Astrology       1.00      0.75      0.86         4\n",
      "       Cities       1.00      0.50      0.67         6\n",
      "      Cricket       0.67      0.25      0.36         8\n",
      "   Editorials       1.00      0.10      0.18        10\n",
      "    Education       0.14      0.88      0.25         8\n",
      "Entertainment       0.00      0.00      0.00         5\n",
      "         Home       0.00      0.00      0.00         8\n",
      "        India       0.25      0.20      0.22         5\n",
      "  Latest News       0.00      0.00      0.00         8\n",
      "    Lifestyle       0.00      0.00      0.00         4\n",
      "        World       0.00      0.00      0.00         5\n",
      "\n",
      "     accuracy                           0.24        71\n",
      "    macro avg       0.37      0.24      0.23        71\n",
      " weighted avg       0.39      0.24      0.21        71\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Assuming df_result is the DataFrame with 'article' and 'section' columns\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data = train_test_split(df_result, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature extraction using TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X_train = vectorizer.fit_transform(train_data['article'])\n",
    "X_test = vectorizer.transform(test_data['article'])\n",
    "\n",
    "# Create and train the text classification model (Random Forest Classifier)\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "classifier.fit(X_train, train_data['section'])\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(classifier, 'text_classifier_model.joblib')\n",
    "\n",
    "# Predictions on the test dataset\n",
    "predictions = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(test_data['section'], predictions)\n",
    "classification_rep = classification_report(test_data['section'], predictions)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print('\\nClassification Report:\\n', classification_rep)\n",
    "\n",
    "# Create a DataFrame for test evaluation\n",
    "eval_df = pd.DataFrame({\n",
    "    'Actual': test_data['section'],\n",
    "    'Predicted': predictions\n",
    "})\n",
    "\n",
    "# Save the evaluation report to a CSV file\n",
    "eval_df.to_csv('evaluation_report.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb0d0d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
